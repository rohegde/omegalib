= Opegalib Concept =
This page offers a general overview of the omegalib structure, presenting some of its main functionality.

<img src="http://omegalib.googlecode.com/svn/wiki/architecture.png" width="600"/>

Inside Omegalib, hardware abstraction is implemented through two concepts: _display system abstraction_ and _input system abstraction_.

== Display System Abstraction ==
Omegalib manages rendering using the concept of display systems: A display system takes care of setting up the graphical hardware system, creating windows and viewports, setting up transformations and rendering pipelines and calling the appropriate application-level rendering functions. Currently, two display systems have been implemented: a simple GLUT based display system used mainly for debug purposes, and an Equalizer based display system.

<a href="http://www.equalizergraphics.com/">Equalizer</a> is a toolkit for scalable parallel rendering based on OpenGL. It allows users to develop scalable graphics applications for a wide range of systems ranging from large distributed visualization clusters and multi-processor multipipe graphics systems to single-processor single-pipe desktop machines. In the near future, we are considering the introduction of a new display system to support autostereoscopic displays based on active parallax barriers, like the Dynallax [18].

The separation between rendering management and the actual application rendering code allowed us to support the concept of rendering layers. Layers represent conceptually separate sets of graphical primitives (for instance a 3D scene and a 2D interface) that can be enabled or disabled for specific output channels of the display system. In this way, it is very easy to implement separate 3D views for the same application, or create a management window running on a secondary display, showing an administration UI or a debug-mode scene rendering.

It is also possible to perform rendering of layers on separate threads, and compose them in the target channel frame buffer: this can be used to make the rendering performance of 2D elements of the application independent from the complexity of the 3D scene, in order to maintain a good frame rate and responsiveness on the UI as the visualized scene grows in complexity.

== Input Device Abstraction ==
Omegalib gives applications access to input devices through the concept of _event services_: an event service manages one physical or logical event source in the system. For instance it can:
 * offer access to events from a real input device, like a touch display or a motion capture system;
 * receive events from a remote source through a network connection;
 * generate input from a logical source, like a user interface button or slider;
 * process events from other sources to act as a _background utility service_. For example, a service can get position data for the user head from a tracking or motion capture service, update the observer head matrices for a scene and send the application updates on the user tracking status).
Event services allow for a great deal of flexibility. They abstract the physical input devices available to the system. Also, they allow to modularize several common components of a virtual reality application (like user tracking or network message passing), so that they can easily be reused in applications.

<img src="http://omegalib.googlecode.com/svn/wiki/oinputserver.png" width="600"/>

Omegalib also supports the streaming of events to external applications, acting as a display-less input server. This simplifies the development of OmegaDesk applications using differents toolsets (as Unity or Processing) and streamlines the integration of input support into legacy applications that treat the device displays as normal screens, but want to use the motion capture, tracking or multitouch capabilities of OmegaDesk.

== Configuration ==
Similar to other VR libraries, Omegalib allows applications to be reconfigured using system description files: display system, event service and application parameters are all stored in configuration files: the same application can run on OmegaDesk with head and hand tracking, on a multitouch tiled display without stereo support, or on a developer laptop using just mouse and keyboard inter-action.

== Interaction ==
Through use of tracker based mocap, Kinect user tracking and touch screens OmegaDesk offers a wide range of possibilities in terms of user interaction. Different applications may request subsets of the available input devices and implement an interaction scheme that works best for the specific application scenario: in some instances, the motion capture system may be used just for head tracking, while interaction with the application 3D objects can be realized through the touch screen. In other scenarios we may need a full mocap-based interaction scheme, with direct hand manipulation of the 3D objects.

We think a certain, predefined number of interaction metaphors would satisfy most of the interaction needs of final applications. In this case, it makes sense to modularize them and make them available to application developers as packaged interaction schemes that can be easily turned on, off or switched inside an application, allowing for both consistency and reuse of interaction schemes, and fast prototyping of applications using different interaction techniques. To implement this, omegalib offers support for a simple scene graph system based on <a href="http://www.ogre3d.org/">Ogre</a> that can be controlled through interaction objects. These objects implement interaction policies, by getting input from the event services and controlling nodes and objects in the scene graph. 

== Integration With Scientific Visualization Tools ==
<img src="http://omegalib.googlecode.com/svn/wiki/ovtk.png" width="600"/>
One of the purposes of OmegaDesk is to be used as a scientific visualization tool: it is therefore necessary to integrate it with standard tools and libraries, like the <a href="www.vtk.org">Visualization Toolkit (VTK)</a>. Through Omegalib, Omegadesk is able to load VTK pipelines as python scripts, render them through the omegalib display sys-tem and interact with VTK actors and 3D models using the interaction schemes presented in the previous section. VTK python scripts can also create user interface widgets that modify the visualization pipeline, and can be controlled through the touch screen. It is also possible to create VTK programs for OmegaDesk natively, using the C++ VTK API directly. This makes it extremely easy to build VTK programs for OmegaDesk or port legacy pipelines to the system. 
