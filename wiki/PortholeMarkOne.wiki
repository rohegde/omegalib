#summary Developement proposal and design for Porthole, the iOS implementation of OmegaViewer
#sidebar Sidebar
This document contains the Porthole application concept, timeline and developement notes.

<wiki:toc max_depth="2" />

= Introduction and Research Question =

  The Cave Automatic Virtual Environment [http://www.evl.uic.edu/pape/CAVE/ CAVE] and related research has shown that Virtual Environments (VE) can be an effective way to visualize, interact and manipulate scientific data.  Additionally devices that blend 2D/3D content into a single VE workstation, like [http://www.evl.uic.edu/core.php?mod=4&type=3&indi=335 Dynallax], allowing scientist to work in an autostereoscopic VE and a 2D workstation simultaneously.  This merging of 2D/3D content is taken a step further in the [http://www.evl.uic.edu/files/pdf/ISVC11_Febretti.pdf OmegaDesk] where 2D/3D interaction are also used simultaneously.  Additionally, the Scalable Adaptive Graphics Environment [http://www.evl.uic.edu/core.php?mod=4&type=1&indi=281 (SAGE)] and related research has shown that large display environments can be an effective way to collaborate and convey ideas.  With the creation of the [http://www.evl.uic.edu/core.php?mod=4&type=4&indi=782 3D Cyber-commons], visualizations generated from a VE can now be viewed in stereo on a large display environment that is suited for collaboration.  Currently there is no middleware to bridge these two technologies nor is there a simple methodology to transfer this data to a personal computer.

  Porthole is a middleware application that bridges these gaps by abstracting data from a VR application to a portable tablet device for easy manipulating, viewing and transporting. The data transferred to the tablet consist of a specification for a user interface, a stream of rendered images, and geometry data. Porthole allows applications to specify an user interface that is composed of supported GUI elements, namely: buttons, sliders and switches. This specification would help tie a GUI element, a slider, to a manipulation function within the application, a scale model function. These GUI elements in conjunction with multi-touch information generated by the tablet would signal the application to manipulate the data in the visualization. A constant stream of rendered images will give users visual feedback on the tablet device for further manipulations. Once manipulations are complete, geometry data can be transferred to the tablet for transport. Ideally the transportation can be to a number of places: a personal computer where further analysis can be done, a Cyber-commons work area were the data can be distributed to peers and discussed, or another VR device that offers different capabilities.

  This report summarizes our current implementation of Porthole as a middleware application between VR applications and an iPad2 for easy manipulation and viewing. A message passing protocol was established in order to communicate between the VR Application and Porthole. The VR applications tested are all managed by OmegaLib, which handles manipulations, rendering and message passing to Porthole. Porthole uses iOS infrastructure to handle message passing and an interface that incorporates a custom GUI specified by the VR application and rendered images. Given the time allotted, the transportation of the resulting data was not feasible and will be discussed along with the other future works.

= Message Passing Protocol =
  The diagram below outlines all the messages that are passed between the VR Application, running the OmegaAppServer, and Porthole, acting as Client on the iPad. The detailed protocol is outlined in this a pdf that can be downloaded [http://omegalib.googlecode.com/svn/wiki/PortholeMsgPassProtocol.pdf" here].

<p align="middle"><img src="http://omegalib.googlecode.com/svn/wiki/portholeMsgPassing.png" width="450"/></p><p align="middle">
_^This is a generic outline of the messages exchanged between Porthole and the VR Application.^_
</p>

= VR Applications =
  The VR Application incorporate OmegaLib.  OmegaLib handle setting up the VR scene and generating the correct views.  One of the views will be polarized passive stereo for the VR device of choice and the other the view for the iPad for user feedback.  All device input will be abstracted by OmegaLib in order to manipulate the scene.  This includes the mouse and keyboard, the iPad GUI element inputs, iPad multi-touch input, and head tracker data.  
  Implementing a VR Application was fairly straight forward.  However generating the correct views for both the VR device and iPad simultaneously proved to be a bit of a challenge.  This will be elaborate on more shortly.

= Porthole Application =
  The Porthole Application has two main areas: the Free Interaction Area (FIA) and the Custom UI Area (CUA).
The Free Interaction Area allow the user to freely manipulate the data via touch.  The iPad support translation, rotation, pinch and swipe gestures as well as simple touches.  A subset of these were used to demonstrate that manipulations made on the FIA were reflected on the VR Application running on the VR device.  Additionally the FIA had a thumbnail of the data being manipulated as feedback for the user.
The Custom UI Area provided users with specified fileds that would allo for further data manipulation.  This section support iOS buttons, sliders, and switches.  The main hurdle was designing an architecture that fostered communication between the main application loop, FIA , CUA , and the TCP Socket connection.
Overall Arch ?

= Future Works =
  One short term future work would be to clean up this code so that Porthole can be released to the public.  Additionally, a OmegaAppicaitonServerProxy was designed and implemented so in order to test the message passing.  This will also be repackaged as an simple example for users to understand how to comunicate with Porthole from the VR Application side.  

  The longer term future work would be to add in the transportation functionality mentioned in the introduction.  

= List of proposed features =
== Primary Features ==
 * The application will connect to a single iPad
 * The user will be able to choose a 3d model from a list of available ones displayed on the iPad. The model will be visualized on the 3d display
 * The application will support interaction with the iPad to move, rotate and scale the object.  As stated in the Application Concept , this interaction will occur in the Interaction Area.  The supported interactions types will be.
  * single finger moving: object translation on the x, y plane *with respect to the user viewpoint*
  * two finger pushing  in the z direction *with respect to the user viewpoint*
  * rotation
  * pinch zoom to scale the object
 * The user will be able to open a secondary user interface on the iPad, and modify properties of the displayed model or of the scene. Supported properties:
  * The model draw mode (wireframe, shaded, wireframe + shaded)
  * The model material properties
  * The placement of lights in the scene and their color.
 * At any time, the user will be able to switch to another model.
 * The entire system will be head tracked. Object manipulation will always happen with respect to the user view point.

== Wishlist / Future work ==
 * In addition to the UI, the tabled displays a dynamic thumbnail of the 3d model or some other image streamed by the VR system.
 * In addition to interaction via iPad, game controller interaction to navigate around the model will be supported
 * Interaction with multiple models in the scene
  * The user can display a set of models in the scene and interact with them separately.
  * Multiple users can use separate iPad to interact with objects
 * Loading of more complex models (i.e. Vtk visualizations)
 * Interaction with complex model parameters (i.e. controlling a vtk pipeline)
 * Multiple display surfaces controlled by the system (hyundai + samsung)
 
= One Page Project Management (OPPM) for OmegaViewer =
https://docs.google.com/spreadsheet/ccc?key=0Ak41PBgfmhF7dEM5ZEVpTTJZVThLTjhOTjdzM0tOUnc&hl=en_US#gid=0\end{abstract}