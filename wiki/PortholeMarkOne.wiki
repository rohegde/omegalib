#summary Developement proposal and design for Porthole, the iOS implementation of OmegaViewer
#sidebar Sidebar
This document contains the Porthole application concept, timeline and developement notes.

<wiki:toc max_depth="2" />

= Introduction and Research Question =

  The Cave Automatic Virtual Environment [http://www.evl.uic.edu/pape/CAVE/ CAVE] and related research has shown that Virtual Environments (VE) can be an effective way to visualize, interact and manipulate scientific data.  Additionally devices that blend 2D/3D content into a single VE workstation, like [http://www.evl.uic.edu/core.php?mod=4&type=3&indi=335 Dynallax], allowing scientist to work in an autostereoscopic VE and a 2D workstation simultaneously.  This merging of 2D/3D content is taken a step further in the [http://www.evl.uic.edu/files/pdf/ISVC11_Febretti.pdf OmegaDesk] where 2D/3D interaction are also used simultaneously.  Additionally, the Scalable Adaptive Graphics Environment [http://www.evl.uic.edu/core.php?mod=4&type=1&indi=281 (SAGE)] and related research has shown that large display environments can be an effective way to collaborate and convey ideas.  With the creation of the [http://www.evl.uic.edu/core.php?mod=4&type=4&indi=782 3D Cyber-commons], visualizations generated from a VE can now be viewed in stereo on a large display environment that is suited for collaboration.  Currently there is no middleware to bridge these two technologies nor is there a simple methodology to transfer this data to a personal computer.

  Porthole is a middleware application that bridges these gaps by abstracting data from a VR application to a portable tablet device for easy manipulating, viewing and transporting. The data transferred to the tablet consist of a specification for a user interface, a stream of rendered images, and geometry data. Porthole allows applications to specify an user interface that is composed of supported GUI elements, namely: buttons, sliders and switches. This specification would help tie a GUI element, a slider, to a manipulation function within the application, a scale model function. These GUI elements in conjunction with multi-touch information generated by the tablet would signal the application to manipulate the data in the visualization. A constant stream of rendered images will give users visual feedback on the tablet device for further manipulations. Once manipulations are complete, geometry data can be transferred to the tablet for transport. Ideally the transportation can be to a number of places: a personal computer where further analysis can be done, a Cyber-commons work area were the data can be distributed to peers and discussed, or another VR device that offers different capabilities.

  This report summarizes our current implementation of Porthole as a middleware application between VR applications and an iPad2 for easy manipulation and viewing. A message passing protocol was established in order to communicate between the VR Application and Porthole. The VR applications tested are all managed by OmegaLib, which handles manipulations, rendering and message passing to Porthole. Porthole uses iOS infrastructure to handle message passing and an interface that incorporates a custom GUI specified by the VR application and rendered images. Given the time allotted, the transportation of the resulting data was not feasible and will be discussed along with the other future works.


= Message Passing Protocol =
  The diagram below outlines all the messages that are passed between the VR Application, running the OmegaAppServer, and Porthole, acting as Client on the iPad.

Outline the message passing with a diagram
Order of the message passing.
Give a link to the protocol

= VR Applications =
Talk about mesh viewer?
OpenSceneGraph?
When you passed a img mesh?
When you passed a GUI mesh?
The VR system will be driven by omegalib + Equalizer in order to control all 4 displays and 2 GPUs available in the system. 
The rendering load will be distributed between the Gpus, allowing to display fairly complex models?

= Porthole Application =
The layout of the application 

 # *Free Interaction Area*: This will allow the user to freely manipulate the data via touch.  Interactions such as translation, rotation and scaling will be implemented using iPad's multitouch capability.  Implementing the Free Interaction Area is the stepping stone to answering the above research questions.  Additionally it will demonstrate that the subtleties of a VR system are understood by this group.

 # *Custom UI*: The UI is determined based on the type of parameters used to alter the data/model. These specification, like the data/model itself, will be given by the user.  Currently it is unsure whether this will be done programmatically or via a configuration file.

Overall Arch ?

= Future Works =


= List of proposed features =
== Primary Features ==
 * The application will connect to a single iPad
 * The user will be able to choose a 3d model from a list of available ones displayed on the iPad. The model will be visualized on the 3d display
 * The application will support interaction with the iPad to move, rotate and scale the object.  As stated in the Application Concept , this interaction will occur in the Interaction Area.  The supported interactions types will be.
  * single finger moving: object translation on the x, y plane *with respect to the user viewpoint*
  * two finger pushing  in the z direction *with respect to the user viewpoint*
  * rotation
  * pinch zoom to scale the object
 * The user will be able to open a secondary user interface on the iPad, and modify properties of the displayed model or of the scene. Supported properties:
  * The model draw mode (wireframe, shaded, wireframe + shaded)
  * The model material properties
  * The placement of lights in the scene and their color.
 * At any time, the user will be able to switch to another model.
 * The entire system will be head tracked. Object manipulation will always happen with respect to the user view point.

== Wishlist / Future work ==
 * In addition to the UI, the tabled displays a dynamic thumbnail of the 3d model or some other image streamed by the VR system.
 * In addition to interaction via iPad, game controller interaction to navigate around the model will be supported
 * Interaction with multiple models in the scene
  * The user can display a set of models in the scene and interact with them separately.
  * Multiple users can use separate iPad to interact with objects
 * Loading of more complex models (i.e. Vtk visualizations)
 * Interaction with complex model parameters (i.e. controlling a vtk pipeline)
 * Multiple display surfaces controlled by the system (hyundai + samsung)
 
= One Page Project Management (OPPM) for OmegaViewer =
https://docs.google.com/spreadsheet/ccc?key=0Ak41PBgfmhF7dEM5ZEVpTTJZVThLTjhOTjdzM0tOUnc&hl=en_US#gid=0\end{abstract}